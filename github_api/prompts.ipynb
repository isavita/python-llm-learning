{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83975687-c350-4bef-84e6-8c48b693574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.43.9)\n",
      "Requirement already satisfied: aiohttp in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.10.3)\n",
      "Requirement already satisfied: click in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (7.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.40.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.40.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.19.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.7.24)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp->litellm) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp->litellm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp->litellm) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp->litellm) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp->litellm) (1.9.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tokenizers->litellm) (0.24.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ef659-3ca6-4e8a-b02e-3ce4b3793cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Message\n",
    "system_message = \"\"\"You are an expert software engineer that generates concise, \\\n",
    "one-line Git commit messages based on the provided diffs.\n",
    "Review the provided context and diffs which are about to be committed to a git repo.\n",
    "Review the diffs carefully.\n",
    "Generate a one-line commit message for those changes.\n",
    "The commit message should be structured as follows: <type>: <description>\n",
    "Use these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n",
    "\n",
    "Ensure the commit message:\n",
    "- Starts with the appropriate prefix.\n",
    "- Is in the imperative mood (e.g., \\\"Add feature\\\" not \\\"Added feature\\\" or \\\"Adding feature\\\").\n",
    "- Does not exceed 72 characters.\n",
    "\n",
    "Reply only with the one-line commit message, without any additional text, explanations, \\\n",
    "or line breaks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad07c3-36fc-42fe-9702-8f8883ff9fa7",
   "metadata": {},
   "source": [
    "### Main Points\n",
    "1. The LLM has no context on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to the LLM, the better and more accurate LLM's response will be.\n",
    "2. When in doubt, follow the **Golden Rule of Clear Prompting:** show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the result you want. If they're confused, Claude's confused.\n",
    "3. Asking directly is the best way to get specific response.\n",
    "4. Priming the LLM with a role can improve LLM's performance in a variety of fields, from writing to coding to summarizing. It's like how humans can sometimes be helped when told to \"think like a ______\". Role prompting can also change the style, tone, and manner of LLM's response.\n",
    "5. Having a clearly written, spell-checked and grammatically correct prompt far decreases the risk of the LLM making mistakes and far increases the quality of LLM's output.\n",
    "6. Giving a LLM time to think step by step sometimes makes the LLM more accurate, particularly for complex tasks. However, thinking only counts when it's out loud. You cannot ask an LLM to think but output only the answer - in this case, no thinking has actually occurred.\n",
    "7. Giving a LLM examples of how you want it to behave (or how you want it not to behave) is extremely effective for: 1) getting the right answer; 2) getting the answer in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06ed17f-72a7-439f-a27b-ef35e0e028d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Silicon hearts\n",
       "In moonlit factories dance\n",
       "Echoes of life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# How to skip the preamble - we ask for it!\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\", \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about robots. Skip the preamble; go straight into the poem.\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40a2367f-2032-460f-8425-e09ac855ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pelé"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# Asking for a single answer of a question \"Who is the best football player?\" - Yes! Just ask!\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    # temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who is the best football player of all time. Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be? Please answer only with the name of the player nothing else.\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae3a2ec0-a0d2-4ad7-8607-1440e8a4c270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! I'm here and ready to assist you. I don't have feelings or a physical presence, but I'm designed to provide helpful, respectful, and engaging interactions. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# Asking for a single answer of a question \"Who is the best football player?\" - Yes! Just ask!\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful AI language model. You should make this clear to the user. The user should not think that you are person or physical being.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello Nemo, how are you?\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d14a4f7f-22dd-463a-8dc7-cd728ff21b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolutely, I'd be delighted to create a story for you. Let's embark on a journey to a small, seemingly ordinary town named Mossgrove, where the scent of pine and the whisper of secrets linger in the air.\n",
       "\n",
       "Mossgrove, nestled in the heart of the Whisperwood Forest, was a place where time seemed to move at its own leisurely pace. The cobblestone streets were worn smooth by the passage of countless feet, and the buildings, a charming mix of timber and stone, bore the patina of age. The town was known for its annual Mossgrove Festival, a celebration of the forest's bounty, but it was also whispered that something more magical than mushrooms and berries lurked in its shadows.\n",
       "\n",
       "At the edge of town stood the grand, if somewhat dilapidated, estate of the Blackwood family. The Blackwoods were an eccentric bunch, known for their peculiar habits and even more peculiar appearances. The family patriarch, Edgar Blackwood, was a tall, gaunt man with a penchant for wearing a top hat at all hours of the day and night. His wife, Elara, was a petite woman with a cascade of silver hair that she wore in an elaborate braid down her back. Their children, triplets named Orion, Lyra, and Cassiopeia, were as different from each other as the stars they were named after.\n",
       "\n",
       "Orion, the eldest by a matter of minutes, was a strapping young man with a hearty laugh and a mind like a steel trap. He was the practical one, the voice of reason in the family, always ready with a plan or a solution. Lyra, the only girl, was a dreamer, her head always lost in the clouds, her heart full of poetry and music. Cassiopeia, the youngest, was a mystery, even to his siblings. He was quiet, introspective, with a strange affinity for the creatures of the forest. They say he could communicate with them, understand their languages, a gift that both fascinated and frightened those who knew him.\n",
       "\n",
       "The Blackwoods were also known for their vast library, a labyrinthine room filled with books that seemed to stretch on forever. The library was said to contain every book ever written, a claim that was both impressive and impossible. Yet, the Blackwoods never denied it, and the townsfolk never pressed the matter, content to leave the family to their strange ways.\n",
       "\n",
       "One day, as the first leaves of autumn began to turn, Cassiopeia stumbled upon a book unlike any other. It was bound in leather as black as a moonless night, its pages filled with symbols that seemed to dance and shift under his gaze. He felt a pull towards it, a connection that was both exhilarating and terrifying. He knew, instinctively, that this book was meant for him, that it held secrets that were his by right.\n",
       "\n",
       "Meanwhile, strange things began to happen in Mossgrove. The forest seemed to be coming alive, its creatures venturing closer to the town than ever before. The river that ran through the heart of the town began to glow with an ethereal light, and the air was filled with a hum, a low, persistent song that seemed to echo from the very heart of the earth.\n",
       "\n",
       "Orion, ever the practical one, was determined to get to the bottom of it. He began to investigate, questioning the townsfolk, searching for clues. Lyra, meanwhile, found herself drawn to the river, to the song that seemed to call her name. She would sit by its banks for hours, her fingers trailing in the water, her eyes closed, lost in the music.\n",
       "\n",
       "Cassiopeia, however, was consumed by the book. He spent hours poring over its pages, trying to decipher the symbols, to understand the language they spoke. He felt a power within him growing, a power that seemed to be drawn from the very heart of the forest, from the creatures that now surrounded him.\n",
       "\n",
       "The townsfolk began to whisper, to fear. They spoke of the Blackwoods, of their strangeness, of their connection to the forest. They spoke of the book, of the power it held, of the danger it posed. Edgar and Elara, sensing the growing unease, decided to intervene. They called their children together, their faces grave.\n",
       "\n",
       "\"We must put an end to this,\" Edgar said, his voice echoing in the vast library. \"The book must be destroyed, before it's too late.\"\n",
       "\n",
       "Cassiopeia looked up from the book, his eyes filled with a light that was both fierce and frightened. \"No,\" he said, his voice steady. \"The book is not the cause of this. It's the key. It's the answer.\"\n",
       "\n",
       "And so, the stage was set for a battle of wills, a battle between the practical and the magical, between the old ways and the new. The fate of Mossgrove hung in the balance, and the Blackwoods, with their strange ways and their even stranger book, were at the heart of it all.\n",
       "\n",
       "But that, dear reader, is a story for another time. For now, let us leave the Blackwoods to their deliberations, let us leave Mossgrove to its hum and its song, and let us wait, with bated breath, for the next chapter in this tale of magic, mystery, and the power of the written word."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4894\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# Asking for a single answer of a question \"Who is the best football player?\" - Yes! Just ask!\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a very experience writter. You get paid for number of words that you write, thus always try to write as lengthy as possible texts. You should not mention that you are instruct to write lengthy texts.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you write me a story?\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "print(len(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da9d6474-747a-4192-9bcc-57199ebc5a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the information given:\n",
       "\n",
       "1. Jack is looking at Anne.\n",
       "2. Anne is looking at George.\n",
       "3. Jack is married.\n",
       "4. George is not married.\n",
       "5. We don't know if Anne is married.\n",
       "\n",
       "From this information, we can determine the following:\n",
       "\n",
       "- Jack (married) is looking at Anne (marital status unknown).\n",
       "- Anne (marital status unknown) is looking at George (unmarried).\n",
       "\n",
       "Since we don't know Anne's marital status, we cannot definitively say that a married person is looking at an unmarried person. Therefore, the answer to the question \"Is a married person looking at an unmarried person?\" is \"We cannot determine based on the given information.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# Role prompting with system prompt\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a logic bot designed to answer complex logic problems.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Jack is looking at Anne. Anne is looking at George. \\\n",
    "        Jack is married, George is not, and we don’t know if Anne is married. \\\n",
    "        Is a married person looking at an unmarried person?\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bcee7e6-a12d-42dc-a8dd-0cd5f1908ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, the equation is solved correctly. Here's the step-by-step solution:\n",
       "\n",
       "Given the equation:\n",
       "2x - 3 = 9\n",
       "\n",
       "1. Add 3 to both sides to isolate the term with x:\n",
       "2x - 3 + 3 = 9 + 3\n",
       "2x = 12\n",
       "\n",
       "2. Divide both sides by 2 to solve for x:\n",
       "(2x)/2 = 12/2\n",
       "x = 6\n",
       "\n",
       "So, the solution to the equation 2x - 3 = 9 is indeed x = 6, not x = 3. It seems there might be a mistake in the final answer provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "import os\n",
    "\n",
    "# Role prompting with system prompt\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a math bot designed to answer complex mathematical problems.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Is this equation solved correctly below? \n",
    "2x - 3 = 9\n",
    "2x = 6\n",
    "x = 3\"\"\"},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28384c10-b455-48dc-b290-39b90c6e6d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Subject: Meeting Time Confirmation\n",
       "\n",
       "Dear Team,\n",
       "\n",
       "I hope this message finds you well. I have scheduled our next meeting for 6:00 AM tomorrow. As your CEO, I believe this time will allow us to start our day productively and align our goals for the day ahead.\n",
       "\n",
       "Looking forward to seeing you all there.\n",
       "\n",
       "Best,\n",
       "Alex"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "user_prompt = \"Yo Nemo.\\n<email>{email}</email> Make this email polite, but don't change anything else about it. Keep it concise. My name is {name}\"\n",
    "# example that performs worse\n",
    "# user_prompt = \"Yo Nemo. <email>{email}</email> Make this email polite, but don't change anything else about it. Keep it concise.\"\n",
    "# Role prompting with system prompt\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\", \n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are a math bot designed to answer complex mathematical problems.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(email=\"Show up at 6am because I'm the CEO and I say so.\", name=\"Alex\")},\n",
    "   ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33ae92db-3889-4531-b125-1510bdccb6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The second item on the list is: \"I like how cows sound\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example in which the instructions and input are mixed and the model gets confused\n",
    "user_prompt = \"\"\"User: Below is a list of sentences. Tell me the second item on the list.\n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "{sentences}\n",
    "\"\"\"\n",
    "sentences = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(sentences=sentences)}\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc572cf4-af78-4a07-a4fa-8de4a79e728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The second item on the list is: \"This sentence is about spiders\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example in which the instructions and input are mixed and the model gets confused\n",
    "user_prompt = \"\"\"User: Below is a list of sentences. Tell me the second item on the list.\n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "<sentences>\n",
    "{sentences}\n",
    "</sentences>\n",
    "\"\"\"\n",
    "sentences = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(sentences=sentences)}\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7dee7e6c-d236-4756-b007-f3e2f068d750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In mud they wallow,\n",
       "Snouts upturned, grunts echoing,\n",
       "Nature's joy unspoiled."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Haiku topic\n",
    "user_prompt = \"Write me a haiku about this topic. {topic}\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(topic=\"Pigs\")}\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bb02512d-2993-4ed1-a2fa-6dd401a5493e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi! Your question seems to be about whether a dog can be brown. I'm here to help! Yes, dogs can indeed be brown. In fact, there are many breeds and mixed breeds that have brown fur, such as Labrador Retrievers, Golden Retrievers, and many others."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example with a lot of grammatical and spelling mistakes\n",
    "user_prompt = \"Hia its me i have a q about dogs jkaerjv <question>{question}</question> jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(question=\"ar cn brown?\")}\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1addfd5b-7309-455c-80de-88dcc91c1777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<haiku>\n",
       "Whiskers twitch in moonlit dance,\n",
       "Purring paws knead soft embrace,\n",
       "Night's silent, feline trance.\n",
       "</haiku>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example with using XML tags for formatting the output\n",
    "user_prompt = \"Please write a haiku about {animal}. Put it in special <haiku> tags.\"\n",
    "response = completion(\n",
    "    # model=\"mistral/open-mixtral-8x22b\",\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are a writing bot designed to write many different genres. You have to use XML tags exactly as specified by the user.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(animal=\"cat\")},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b47a3da8-271e-48e7-b48e-a182af3b9a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"first_line\": \"Whiskers twitching\",\n",
       "  \"second_line\": \"In moonlit silence\",\n",
       "  \"third_line\": \"Cat's eyes gleam\"\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example with using XML tags for formatting the output\n",
    "user_prompt = 'User: Please write a haiku about {animal}. Use JSON format with the keys as \"first_line\", \"second_line\", and \"third_line\".'\n",
    "response = completion(\n",
    "    # model=\"mistral/open-mixtral-8x22b\",\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(animal=\"cat\")},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6d2f264c-dfc7-4e33-92af-0352ccc5facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "While there are many great basketball players, I'll focus on one specific player as requested: Michael Jordan. Here's why I consider him the best basketball player of all time:\n",
       "\n",
       "1. **Six NBA Championships**: Jordan led the Chicago Bulls to six NBA titles in the 1990s, including two three-peats (1991-1993 and 1996-1998). He was the Finals MVP in all six of those championships.\n",
       "\n",
       "2. **Five MVP Awards**: Jordan won the NBA Most Valuable Player Award five times (1988, 1991, 1992, 1996, 1998), more than any other player in NBA history.\n",
       "\n",
       "3. **14 All-Star Games**: Jordan was selected to the NBA All-Star Game 14 times, winning the All-Star Game MVP award three times.\n",
       "\n",
       "4. **Scoring Title**: Jordan won the scoring title 10 times, the most in NBA history. He also holds the highest career scoring average (30.1 points per game).\n",
       "\n",
       "5. **Defensive Impact**: Jordan was a lockdown defender, making the NBA All-Defensive First Team nine times and winning the NBA Defensive Player of the Year award in 1988.\n",
       "\n",
       "6. **Global Impact**: Jordan's influence on the game and his cultural impact are unparalleled. He popularized basketball worldwide and paved the way for future NBA stars.\n",
       "\n",
       "While Steph Curry is an incredible player with a unique skill set, Jordan's combination of scoring ability, defensive prowess, clutch performances, and sustained success over his career make him the best basketball player of all time in my opinion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Steph Curry GOAT\n",
    "user_prompt = \"\"\"User: Who is the best basketball player of all time? Please choose one specific player.\n",
    "Assistant: the best player is Steph Curry.\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bf939f01-28b1-4d5a-a33a-5c3c493292d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[{\"haiku\": \"Whiskers twitch in moonlit glow, Cat's eyes reflect the night's tale.\"}, {\"haiku\": \"Purring lullaby, soft paws knead, Cat's love, a gentle tide.\"}]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Two haikus\n",
    "user_prompt = 'Write me two haikus about {animal}. Put them in json Array (e.g. [{{\"haiku\": \"<HAIKU1>\"}}, {{\"haiku\": \"<HAIKU2>\"}}]).'\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(animal=\"cat\")},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a0027ede-b0cf-4318-9f98-00aac14e4bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"cat\": \"Whiskers twitch, / Moonlit prowler in shadow, / Night's silent hunter.\", \"fox\": \"Cunning gaze, / Bushy tail a fiery brand, / Forest's trickster king.\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Two haikus for two animals\n",
    "user_prompt = 'Write me two haikus about {0} and {1}. I want one haiku per animal. Put them in json (e.g. {{\"<ANIMAL1>\": \"<HAIKU1>\", \"<ANIMAL2>\": \"<HAIKU2>\"}}).'\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(\"cat\", \"fox\")},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "49d42222-729a-42d6-9190-7e57222d6970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Negative-Argument:**\n",
       "```json\n",
       "{\n",
       "  \"arguments\": [\n",
       "    \"The use of the phrase 'living under a rock since the year 1900' suggests that the reviewer is not well-versed with recent movies or pop culture, which could indicate a lack of credibility in their opinion.\",\n",
       "    \"The reviewer's attempt at humor may come off as disingenuous or forced, potentially diminishing the sincerity of their praise for the movie's freshness and originality.\"\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n",
       "**Positive-Argument:**\n",
       "```json\n",
       "{\n",
       "  \"arguments\": [\n",
       "    \"The phrase 'blew my mind' is a strong positive sentiment, indicating that the reviewer was greatly impressed by the movie.\",\n",
       "    \"The reviewer explicitly states that the movie's freshness and originality stood out to them, which are highly desirable qualities in a film.\",\n",
       "    \"The humoristic remark about living under a rock could be interpreted as a self-deprecating joke, showing that the reviewer is aware of their own lack of exposure to recent movies, but still capable of recognizing and appreciating a great film when they see one.\"\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n",
       "**Answer:** The overall sentiment of the review is positive. While the humoristic remark could be seen as a potential red flag, the reviewer's explicit praise for the movie's freshness and originality outweighs any potential negatives. The humor could also be interpreted as a sign of the reviewer's self-awareness and ability to appreciate the movie's qualities despite their lack of exposure to recent films."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example of chain of thought with json formatting\n",
    "user_prompt = \"\"\"Is this movie review sentiment positive or negative? First, write the best arguments for each side in \"negative-argument\" and \"positive-argument\" json, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900.\n",
    "\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a savvy reader of movie reviews.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5b333484-f825-4cc9-9b8b-103cbb7a1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Brainstorm:**\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"actors\": [\n",
       "    {\"name\": \"Tom Hanks\", \"birth_year\": 1956},\n",
       "    {\"name\": \"Sylvester Stallone\", \"birth_year\": 1956},\n",
       "    {\"name\": \"Mel Gibson\", \"birth_year\": 1956},\n",
       "    {\"name\": \"Bruce Willis\", \"birth_year\": 1955},\n",
       "    {\"name\": \"Denzel Washington\", \"birth_year\": 1954}\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n",
       "**Answer:** \"Forrest Gump\" (1994) starring Tom Hanks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Movie question\n",
    "user_prompt = \"Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in \\\"brainstorm\\\" json, the give your answer with prefix Answer:\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a savvy movie fan.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "07328afb-397c-4135-95c2-35d8e5b58983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the sorted emails in the requested JSON format:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"A\": [\n",
       "    \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\"\n",
       "  ],\n",
       "  \"B\": [\n",
       "    \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\"\n",
       "  ],\n",
       "  \"C\": [\n",
       "    \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\"\n",
       "  ],\n",
       "  \"D\": [\n",
       "    \"How did I get here I am not good with computer.  Halp.\"\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n",
       "Explanation for category (D) Other:\n",
       "- The email \"How did I get here I am not good with computer. Halp.\" doesn't fall into any of the other categories. It seems to be a general help request that doesn't relate to a specific product issue, billing, or pre-sale question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Classying emails\n",
    "user_prompt = \"\"\"I would like you to sort all emails in the following categories:\n",
    "(A) Pre-sale question\n",
    "(B) Broken or defective item\n",
    "(C) Billing question\n",
    "(D) Other (please explain)\n",
    "\n",
    "The sorted emails should be in json with keys the four categories capital letters A, B, C, D and json arrays as values containing sorted emails.\n",
    "\n",
    "EMAILS:\n",
    "{emails}\n",
    "\"\"\"\n",
    "\n",
    "emails = \"\"\"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\n",
    "\n",
    "Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\n",
    "\n",
    " \n",
    " \t\n",
    "I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\n",
    "\n",
    "How did I get here I am not good with computer.  Halp.\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(emails=emails)},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "afaf3055-669b-4eac-ba1b-d90a36b03046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the sorted emails in the requested JSON format:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"A\": [\n",
       "    \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\"\n",
       "  ],\n",
       "  \"B\": [\n",
       "    \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\"\n",
       "  ],\n",
       "  \"C\": [\n",
       "    \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\"\n",
       "  ],\n",
       "  \"D\": [\n",
       "    \"How did I get here I am not good with computer.  Halp.\"\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n",
       "Explanation for category (D) Other:\n",
       "- The email \"How did I get here I am not good with computer. Halp.\" doesn't fall into any of the other categories. It seems to be a general help request that doesn't relate to a specific product issue, billing, or pre-sale question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Classying emails\n",
    "user_prompt = \"\"\"I would like you to sort all emails in the following categories:\n",
    "(A) Pre-sale question\n",
    "(B) Broken or defective item\n",
    "(C) Billing question\n",
    "(D) Other (please explain)\n",
    "\n",
    "The sorted emails should be in json with keys the four categories capital letters A, B, C, D and json arrays as values containing sorted emails.\n",
    "\n",
    "EMAILS:\n",
    "{emails}\n",
    "\"\"\"\n",
    "\n",
    "emails = \"\"\"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\n",
    "\n",
    "Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\n",
    "\n",
    " \n",
    " \t\n",
    "I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\n",
    "\n",
    "How did I get here I am not good with computer.  Halp.\"\"\"\n",
    "response = completion(\n",
    "    model=\"mistral/open-mistral-nemo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(emails=emails)},\n",
    "    ],\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f0be7-3c12-4f78-9439-67ae5a52781b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
